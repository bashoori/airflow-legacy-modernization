# ğŸš€ Airflow Legacy Job Modernization & ETL Pipeline

This project demonstrates how to migrate legacy **Windows Task Scheduler** jobs to modern, reliable **Apache Airflow DAGs**, containerized with **Docker**, integrated with **AWS-style ETL logic**, and deployed using **GitHub Codespaces** or local Docker Compose.

It includes secure config with `.env`, modular Python scripts, automated email alerts, and a mock ETL pipeline ready for production scaling.

---

## ğŸ§­ Project Goals

- âœ… Migrate legacy batch jobs to **Apache Airflow DAGs**
- âœ… Build clean, modular **ETL pipelines** (mock S3 â†’ transform â†’ Redshift)
- âœ… Use `.env` for secret management (email, AWS, DB)
- âœ… Set up **EmailOperator** for failure alerts
- âœ… Containerize the entire setup using **Docker Compose**
- âœ… Enable CI/CD DAG validation via **GitHub Actions**
- âœ… Prepare for real-world Data Engineering roles (like Intermediate Airflow Developer)

---

## ğŸ—‚ Project Structure
```
.
â”œâ”€â”€ dags/                  # Airflow DAGs (3 total)
â”‚   â”œâ”€â”€ legacy_to_airflow_dag.py
â”‚   â”œâ”€â”€ etl_to_redshift.py
â”‚   â””â”€â”€ email_alert_test.py
â”œâ”€â”€ scripts/               # Reusable data transformation logic
â”‚   â””â”€â”€ transform.py
â”œâ”€â”€ data/                  # Sample input and expected output
â”‚   â”œâ”€â”€ sample_raw_data.csv
â”‚   â””â”€â”€ expected_output.csv
â”œâ”€â”€ docker/                # Docker Compose + .env
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ .env
â”œâ”€â”€ .github/workflows/     # CI pipeline
â”‚   â””â”€â”€ deploy_airflow.yml
â””â”€â”€ README.md
```
---

## ğŸ” DAGs Overview

### 1ï¸âƒ£ `legacy_to_airflow_dag.py`
Simulates a 3-step legacy job:
- Download CSV
- Transform (mock logic)
- Upload to S3 (mock)
- Sends email alert on failure (via `EmailOperator`)

### 2ï¸âƒ£ `etl_to_redshift.py`
Daily ETL pipeline:
- Extract from local CSV (mock S3)
- Clean data via `scripts/transform.py`
- Load into Redshift (simulated with logging)

### 3ï¸âƒ£ `email_alert_test.py`
Intentionally fails a task to trigger your SMTP email alert.

---

## ğŸ” .env Configuration

Used across all scripts and Docker services. Example values:

```env
# Email Alerts
ALERT_EMAIL=your_email@gmail.com
AIRFLOW__SMTP__SMTP_USER=your_email@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=your_password
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_PORT=587

# AWS (Mocked)
S3_BUCKET_NAME=my-bucket-name
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret

ğŸ³ Running Locally with Docker
```
cd docker
docker-compose up --build
```
	â€¢	Access Airflow at: http://localhost:8080
	â€¢	Default credentials: airflow / airflow

ğŸ§ª Sample Dataset

sample_raw_data.csv
```
customer_id,full_name,email,signup_date,amount_spent
101,Alice Smith,alice@example.com,2023-11-15,250.75
...
```
Transform logic:
	â€¢	Removes incomplete rows
	â€¢	Adds processed_at timestamp
	â€¢	Saves as final_output.csv

â¸»

âœ… GitHub Actions CI

On each push, GitHub Actions:
	â€¢	Validates all DAGs for syntax errors using py_compile
	â€¢	Prevents broken DAGs from being merged


ğŸ“Š Visual Diagram
```
Windows Legacy Job
      |
      v
Apache Airflow DAGs
 (Dockerized)

â”œâ”€â”€ Download Task
â”œâ”€â”€ Transform Task (Pandas)
â”œâ”€â”€ Upload Task (Mock S3)
â””â”€â”€ Email Alert on Failure

Additional DAG â†’ ETL to Redshift (Mock)
```

```
                +-----------------------+
                |   Windows Task Job    |
                |  (Simulated Legacy)   |
                +----------+------------+
                           |
                           v
                +----------+-----------+
                |   Apache Airflow     |     (via Docker Compose)
                |    Webserver & DAGs  |
                +----------+-----------+
                           |
      +--------------------+---------------------+
      |                    |                     |
      v                    v                     v
+-------------+    +---------------+     +----------------+
|  Download   | -> |  Transform    | --> |   Upload to    |
|  Task       |    |  Task         |     |     S3 (Mock)  |
+-------------+    +---------------+     +----------------+
   (mock S3)          (clean_data)             (Log only)

      |
      v
  Email Alert on Failure
  via EmailOperator
        |
        v
+--------------------+
|   Email: ALERT_EMAIL   |
+--------------------+


        â¤· Additional Pipeline:
        ----------------------
                +
                |
                v
        +------------------------+
        | ETL to Redshift DAG    |
        +------------------------+
                |
         +------+-------+
         |              |
         v              v
   Extract CSV      Transform CSV
    (mocked S3)      (dropna, timestamp)
         |              |
         +------+-------+
                |
                v
         +--------------+
         |  Load to      |
         | Redshift (Sim)|
         +--------------+
         ```






ğŸ™‹ About the Author

Bita Ashoori
ğŸ“ Vancouver, Canada
ğŸ”— LinkedIn
ğŸ”— Portfolio

â¸»

ğŸ“œ License

This project is licensed under the MIT License.

â¸»
